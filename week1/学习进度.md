# 20200706

## 学习进度

1. 安装`WSL`并完成美化（`zsh`/`oh-my-zsh`...）
2. 安装`anaconda3`
3. 更换`conda`源、`pip`源
4. 安装`pytorch`、`TensorFlow`
5. 整理博客文件夹并修改部分主题

## 参考资料

[机器学习中如何处理缺失数据](https://www.zhihu.com/question/26639110)

# 20200707

## 学习进度

1. 学习数据的缺失值处理
2. 初步了解Pima数据集
3. 了解标准化、归一化方法

## 遇到问题

1. 如何处理数据的缺失值？
2. 数据的标准化、归一化方法有哪些？

## 参考资料

[数据挖掘中的缺失值处理](https://qinqianshan.com/math/statics_topic/deal-na/)

[CSDN 数据标准化、归一化 Normalization](https://blog.csdn.net/pipisorry/article/details/52247379)

# 20200708

## 学习进度

1. WSL上安装图形界面，Windows端xlaunch配合使用
2. 熟悉anaconda中JupyterLab以及JupyterNotebook使用
3. 简单了解Google家的Colab
4. 看视频学习CS231n前两节
5. 阅读《统计学习方法》第一章

## 遇到问题

1. 热卡填补(hot-deck)如何选择“相似”的标准？
   1. 可以选择欧式距离判定、也可以通过余弦距离判定
2. EM缺失值填充中极大似然估计是什么？
3. 书中L1范数、L2范数是什么？
4. 损失函数的平方形式看起来好奇怪，为什么是这样？

## 参考资料

[一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)

[L1与L2范数的区别](https://zhuanlan.zhihu.com/p/28023308)

[通俗解释损失函数的平方形式](https://zhuanlan.zhihu.com/p/26171777)

# 20200709

## 学习进度

1. 实现了无剪枝的CART，在网上找到的一个数据集上运行成功
2. 解决了很多在CART实现过程中遇到的问题
3. 对比了机器学习实战中ID3的代码，学习了ID3、C4.5的构建方法

## 遇到问题

1. CART实现
   1. 如何处理离散值数据？One-hot
   2. 如何存储树的结构？Python dict

## 参考资料

[数据预处理：独热编码 One-hot](https://blog.csdn.net/pipisorry/article/details/61193868)

[GitHub上找的别人的代码](https://github.com/flywangfang258/ML_in_action/blob/master/regression_8_9/9_CART_Regression/regTrees.py)

数据集参考：[博客园 CART原理与代码实现](https://www.cnblogs.com/further-further-further/p/9482885.html)

# 20200710

回老家走亲戚，暂停一天

# 20200711

## 学习进度

1. 实现logistic回归的梯度下降法（随机梯度下降、批量梯度下降）
2. 实现logistic回归的牛顿法